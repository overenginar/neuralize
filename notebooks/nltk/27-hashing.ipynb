{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7bdbc72-840d-4c54-a410-83c672d2dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean\n",
    "\n",
    "\n",
    "def get_dict(file_name):\n",
    "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
    "    etof = {}\n",
    "    for i in range(len(my_file)):\n",
    "        en = my_file.loc[i][0]\n",
    "        fr = my_file.loc[i][1]\n",
    "        etof[en] = fr\n",
    "\n",
    "    return etof\n",
    "\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    cos = -10    \n",
    "    dot = np.dot(A, B)\n",
    "    normb = np.linalg.norm(B)\n",
    "    \n",
    "    if len(A.shape) == 1:\n",
    "        norma = np.linalg.norm(A)\n",
    "        cos = dot / (norma * normb)\n",
    "    else:\n",
    "        norma = np.linalg.norm(A, axis=1)\n",
    "        epsilon = 1.0e-9 # to avoid division by 0\n",
    "        cos = dot / (norma * normb + epsilon)\n",
    "        \n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d49b91-a307-404a-a2fe-386f60dbc259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "\n",
    "from os import getcwd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fc752f-c9c5-41a3-9d13-e20ee53fa85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = f\"{getcwd()}/tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f7d0f8-a355-4940-9c2e-1a40cd4150bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "346d696c-3fb7-40c8-a93c-2acdfebd3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"./en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"./fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba19b4d-4797-4233-ad70-360127850a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n",
      "The length of the English to French test dictionary is 1500\n"
     ]
    }
   ],
   "source": [
    "en_fr_train = get_dict('./en-fr.train.txt')\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('./en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e2b481-2a42-4d81-8f99-e4b661a7d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "    english_set = set(english_vecs.keys())\n",
    "    french_set = set(french_vecs.keys())\n",
    "    french_words = set(en_fr.values())\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "            en_vec = english_vecs[en_word]\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "            X_l.append(en_vec)\n",
    "            Y_l.append(fr_vec)\n",
    "    X = np.stack(X_l)\n",
    "    Y = np.stack(Y_l)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0b7b6bd-6316-4bad-94a8-85c49ea1c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    m = X.shape[0]\n",
    "    diff = np.subtract(np.dot(X, R), Y)\n",
    "    diff_squared = np.square(diff)\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "    loss = sum_diff_squared / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "040c4e84-cac5-4edc-b16c-b79d00fc6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    m = X.shape[0]\n",
    "    gradient = 2 * np.dot(X.T, np.subtract(np.dot(X, R), Y)) / m\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8972669-41a5-44d6-b2df-ac1a8f0bb0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003, verbose=True, compute_loss=compute_loss, compute_gradient=compute_gradient):\n",
    "    np.random.seed(129)\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "    for i in range(train_steps):\n",
    "        if verbose and i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "        R -= learning_rate * gradient\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05c07e47-a3e8-42f1-be7e-ed69016228cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):\n",
    "    similarity_l = []\n",
    "    for row in candidates:\n",
    "        cos_similarity = cosine_similarity(v, row)\n",
    "        similarity_l.append(cos_similarity)\n",
    "    sorted_ids = np.argsort(np.stack(similarity_l))\n",
    "    sorted_ids = sorted_ids[::-1]\n",
    "    k_idx = sorted_ids[:k]\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aec9d251-e552-4795-891a-ad61ce5c5b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R, nearest_neighbor=nearest_neighbor):\n",
    "    pred = np.dot(X, R)\n",
    "    num_correct = 0\n",
    "    for i in range(len(pred)):\n",
    "        pred_idx = nearest_neighbor(pred[i], Y)\n",
    "        if pred_idx == i:\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / pred.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dde9216f-00d3-4784-a94d-0d335283fdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 963.0146\n",
      "loss at iteration 25 is: 97.8292\n",
      "loss at iteration 50 is: 26.8329\n",
      "loss at iteration 75 is: 9.7893\n",
      "loss at iteration 100 is: 4.3776\n",
      "loss at iteration 125 is: 2.3281\n",
      "loss at iteration 150 is: 1.4480\n",
      "loss at iteration 175 is: 1.0338\n",
      "loss at iteration 200 is: 0.8251\n",
      "loss at iteration 225 is: 0.7145\n",
      "loss at iteration 250 is: 0.6534\n",
      "loss at iteration 275 is: 0.6185\n",
      "loss at iteration 300 is: 0.5981\n",
      "loss at iteration 325 is: 0.5858\n",
      "loss at iteration 350 is: 0.5782\n",
      "loss at iteration 375 is: 0.5735\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = get_matrices(en_fr_train, fr_embeddings_subset, en_embeddings_subset)\n",
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6080e666-9232-461d-92da-e4e0a0a1c35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.557\n"
     ]
    }
   ],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)\n",
    "acc = test_vocabulary(X_val, Y_val, R_train)\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c2e533f-42ef-4081-ba0f-23c981741ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9a3a6c0-411d-4053-90a1-1861a409bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(tweet, en_embeddings, process_tweet=process_tweet):\n",
    "    doc_embedding = np.zeros(300)\n",
    "    processed_doc = process_tweet(tweet)\n",
    "    for word in processed_doc:\n",
    "        if word not in en_embeddings.keys():\n",
    "            continue\n",
    "        doc_embedding += en_embeddings[word]\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c67210b-9506-4305-bf08-fe45f017d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vecs(all_docs, en_embeddings, get_document_embedding=get_document_embedding):\n",
    "    ind2Doc_dict = {}\n",
    "    document_vec_l = []\n",
    "    for i, doc in enumerate(all_docs):\n",
    "        doc_embedding = get_document_embedding(doc, en_embeddings)\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "        document_vec_l.append(doc_embedding)\n",
    "    document_vec_matrix = np.vstack(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf774b8a-2db7-4df7-b940-1c7d64f7b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e11961e4-a73b-41d6-aa62-c21644e5a883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 10000\n",
      "shape of document_vecs (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of dictionary {len(ind2Tweet)}\")\n",
    "print(f\"shape of document_vecs {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6f710c2-b29f-454d-9366-75693ddff640",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet = 'i am sad'\n",
    "process_tweet(my_tweet)\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9abcc10-e75a-4488-a845-b4780412ee47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@hanbined sad pray for me :(((\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\n",
    "print(all_tweets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "928196cd-4f0e-44c5-948f-8cad8d5b6ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors is 10000 and each has 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "N_VECS = len(all_tweets)\n",
    "N_DIMS = len(ind2Tweet[1])\n",
    "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a1881b6-7f52-4d37-a19b-36f1551bb310",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PLANES = 10\n",
    "N_UNIVERSES = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a47a00e-f61e-439d-aa6b-e3a33b9bc362",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
    "            for _ in range(N_UNIVERSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87d25ec8-8477-431d-85ee-bcde91686b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "    dot_product = np.dot(v, planes)\n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "    h = np.where(sign_of_dot_product >= 0 ,True, False)\n",
    "    h = np.squeeze(h)\n",
    "    hash_value = 0\n",
    "    n_planes = planes.shape[1]\n",
    "    for i in range(n_planes):\n",
    "        hash_value += 2 ** i * h[i]\n",
    "    hash_value = int(hash_value)\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7570a10-f846-4b52-afa9-b5e29361efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes, hash_value_of_vector=hash_value_of_vector):\n",
    "    num_of_planes = planes.shape[1]\n",
    "    num_buckets = 2**num_of_planes\n",
    "    hash_table = {i: [] for i in range(num_buckets)}\n",
    "    id_table = {i: [] for i in range(num_buckets)}\n",
    "    for i, v in enumerate(vecs):\n",
    "        h = hash_value_of_vector(v, planes)\n",
    "        hash_table[h].append(v) # @REPLACE None\n",
    "        id_table[h].append(i) # @REPLACE None\n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e43657d-1cd5-4c9b-b6cf-27cfbd331a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "def create_hash_id_tables(n_universes):\n",
    "    hash_tables = []\n",
    "    id_tables = []\n",
    "    for universe_id in range(n_universes):\n",
    "        print('working on hash universe #:', universe_id)\n",
    "        planes = planes_l[universe_id]\n",
    "        hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "        hash_tables.append(hash_table)\n",
    "        id_tables.append(id_table)\n",
    "    \n",
    "    return hash_tables, id_tables\n",
    "\n",
    "hash_tables, id_tables = create_hash_id_tables(N_UNIVERSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f6a3f82-9294-42b2-ba91-f0ee817e1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_knn(doc_id, v, planes_l, hash_tables, id_tables, k=1, num_universes_to_use=25, hash_value_of_vector=hash_value_of_vector):\n",
    "    vecs_to_consider_l = list()\n",
    "    ids_to_consider_l = list()\n",
    "    ids_to_consider_set = set()\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "        planes = planes_l[universe_id]\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "        hash_table = hash_tables[universe_id]\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "        id_table = id_tables[universe_id]\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "            if doc_id == new_id:\n",
    "                continue\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                document_vector_at_i = document_vectors_l[i]\n",
    "                vecs_to_consider_l.append(document_vector_at_i)\n",
    "                ids_to_consider_l.append(new_id)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
    "                            for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9917ca3a-f48f-44e5-8d02-c3484ea2a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = document_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "766e7d31-482d-4138-84ac-72743123b884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast considering 77 vecs\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbor_ids = approximate_knn(\n",
    "    doc_id, vec_to_search, planes_l, hash_tables, id_tables, k=3, num_universes_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d126f433-2f9a-4732-92c2-426fd1133836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for document 0\n",
      "Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Nearest neighbor at document id 51\n",
      "document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n",
      "Nearest neighbor at document id 2478\n",
      "document contents: #ShareTheLove @oymgroup @musicartisthere for being top HighValue members this week :) @nataliavas http://t.co/IWSDMtcayt\n",
      "Nearest neighbor at document id 105\n",
      "document contents: #FollowFriday @straz_das @DCarsonCPA @GH813600 for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors for document {doc_id}\")\n",
    "print(f\"Document contents: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca9808-bc77-4155-a22d-0be0db0c2dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
