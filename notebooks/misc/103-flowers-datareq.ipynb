{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a40858-0c09-47b7-a25a-67d96f2d31a4",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30bde1fc-d82a-42dc-b6ca-47a894afdaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 14:47:01.615248: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-19 14:47:01.661168: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-19 14:47:01.663208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 14:47:02.691992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c601827e-d77a-4036-bcf8-198923dc1a0d",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d9d784-5cf3-42ee-bfc9-9fb6a0972a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "FILE_DIR = 'datasets/flowers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e40d95-a967-4e52-937c-6cdadba4f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATTERN = FILE_DIR + \"/train*\"\n",
    "EVAL_PATTERN = FILE_DIR + \"/eval*\"\n",
    "\n",
    "\n",
    "def parse_example(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "    example[\"image\"] = tf.image.resize(\n",
    "        example[\"image\"], [IMG_HEIGHT, IMG_WIDTH]\n",
    "    )\n",
    "    example[\"image\"] = example[\"image\"] / 255\n",
    "    return example[\"image\"], example[\"label\"]\n",
    "\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.TFRecordDataset(tf.io.gfile.glob(TRAIN_PATTERN))\n",
    "    .map(parse_example)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "eval_ds = (\n",
    "    tf.data.TFRecordDataset(tf.io.gfile.glob(EVAL_PATTERN))\n",
    "    .map(parse_example)\n",
    "    .batch(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442da0f-67ab-47b2-b15c-9dbc445ba1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_selection = \"mobilenet_v2_100_224\"\n",
    "module_handle = \"https://tfhub.dev/google/imagenet/{}/feature_vector/4\".format(\n",
    "    module_selection\n",
    ")\n",
    "\n",
    "transfer_model = tf.keras.Sequential(\n",
    "    [\n",
    "        hub.KerasLayer(module_handle, trainable=True),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(\n",
    "            len(CLASSES),\n",
    "            activation=\"softmax\",\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transfer_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2ab5a-b1e5-4dfd-ab8b-52bc38f40349",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    validation_data=eval_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98f236-63d2-4327-8b09-f53aaadc0447",
   "metadata": {},
   "source": [
    "### Exporting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c78cab-c350-4115-a3bb-627931e16094",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"export\", ignore_errors=True)\n",
    "os.mkdir(\"export\")\n",
    "transfer_model.save(\"export/flowers_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ca4ba-a225-45bb-9ecb-bb642df5da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir export/flowers_model --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f2eb9-7ae1-466e-b499-c576d445eb48",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e3bfa-9599-4c0f-bbf7-4189fd05053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    \"gs://asl-public/data/flowers/jpegs/10172567486_2748826a8b.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/10386503264_e05387e1f7_m.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/10391248763_1d16681106_n.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/10712722853_5632165b04.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/10778387133_9141024b10.jpg\",\n",
    "    \"gs://asl-public/data/flowers/jpegs/112334842_3ecf7585dd.jpg\",\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess(img_bytes):\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=IMG_CHANNELS)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    return img\n",
    "\n",
    "\n",
    "def read_from_jpegfile(filename):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = preprocess(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "serving_model = tf.keras.models.load_model(\"export/flowers_model\")\n",
    "input_images = [read_from_jpegfile(f) for f in filenames]\n",
    "\n",
    "f, ax = plt.subplots(1, 6, figsize=(15, 15))\n",
    "for idx, img in enumerate(input_images):\n",
    "    ax[idx].imshow(img.numpy())\n",
    "    batch_image = tf.expand_dims(img, axis=0)\n",
    "    batch_pred = serving_model.predict(batch_image)\n",
    "    pred = batch_pred[0]\n",
    "    pred_label_index = tf.math.argmax(pred).numpy()\n",
    "    pred_label = CLASSES[pred_label_index]\n",
    "    prob = pred[pred_label_index]\n",
    "    ax[idx].set_title(f\"{pred_label} ({prob:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d202a-66ac-4b07-9b09-32ee4ca71c14",
   "metadata": {},
   "source": [
    "###Â Additional Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36d610-4f0a-417f-97ab-3f078ebdd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(pred):\n",
    "    top_prob = tf.math.reduce_max(pred, axis=[1])\n",
    "    pred_label_index = tf.math.argmax(pred, axis=1)\n",
    "    pred_label = tf.gather(tf.convert_to_tensor(CLASSES), pred_label_index)\n",
    "\n",
    "    return {\n",
    "        \"probability\": top_prob,\n",
    "        \"flower_type_int\": pred_label_index,\n",
    "        \"flower_type_str\": pred_label,\n",
    "    }\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None,], dtype=tf.string)])\n",
    "def predict_from_filename(filenames):\n",
    "\n",
    "    input_images = tf.map_fn(\n",
    "        read_from_jpegfile, filenames, fn_output_signature=tf.float32\n",
    "    )\n",
    "\n",
    "    batch_pred = transfer_model(input_images)  # same as model.predict()\n",
    "\n",
    "    processed = postprocess(batch_pred)\n",
    "    return processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f52d54-2ad6-494c-8520-c9b990a8d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None,], dtype=tf.string)])\n",
    "def predict_from_b64(img_bytes):\n",
    "\n",
    "    input_images = tf.map_fn(\n",
    "        preprocess, img_bytes, fn_output_signature=tf.float32\n",
    "    )\n",
    "\n",
    "    batch_pred = transfer_model(input_images)  # same as model.predict()\n",
    "\n",
    "    processed = postprocess(batch_pred)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b2d840-ae47-4985-9b3c-eafc529ce068",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.save(\n",
    "    \"export/flowers_model_with_signature\",\n",
    "    signatures={\n",
    "        \"serving_default\": predict_from_filename,\n",
    "        \"predict_base64\": predict_from_b64,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f1a6b-4666-4f0a-85cd-ceadbfbe44f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir export/flowers_model_with_signature --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428bd0a8-b724-44fd-95a0-5e944728951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn = tf.keras.models.load_model(\n",
    "    \"export/flowers_model_with_signature\"\n",
    ").signatures[\"serving_default\"]\n",
    "\n",
    "pred = serving_fn(tf.convert_to_tensor(filenames))\n",
    "\n",
    "for k in pred.keys():\n",
    "    print(f\"{k:15}: {pred[k].numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417e1fd-d5b2-442b-8865-0b43313665f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir export/flowers_model_with_signature --tag_set serve --signature_def predict_base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678fdd35-4d01-4943-97d5-f886c32f7e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
